{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a407116c-0c2f-4709-8b68-d94821226719",
   "metadata": {},
   "source": [
    "# Bagging\n",
    "**Bagging (Bootstrap Aggregating)** is an ensemble learning technique in machine learning designed to improve the stability and accuracy of models. It works by training multiple versions of the same model on different subsets of the training data, generated through bootstrapping (sampling with replacement). Each model independently predicts the outcome, and their results are combinedâ€”typically using averaging for regression or majority voting for classification.\n",
    "\n",
    "Bagging reduces variance, making it particularly effective for high-variance models like decision trees. Random Forest, a popular algorithm, is a direct application of bagging."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "660cc822-7a94-4454-991e-0715f1e19ae1",
   "metadata": {},
   "source": [
    "### **Random Forest Regression and Classification**\n",
    "\n",
    "**Random Forest** is an ensemble learning method built on the principles of decision trees and bagging. It creates a \"forest\" of decision trees, where each tree is trained on a random subset of the data and features. The diversity among the trees helps improve prediction accuracy and reduces overfitting. Here's how it works for regression and classification:\n",
    "\n",
    "1. **Random Forest Regression**  \n",
    "   - In regression tasks, the algorithm predicts continuous values. Each decision tree outputs a numerical prediction, and the final prediction is obtained by averaging the outputs of all trees.\n",
    "   - It excels at handling non-linear relationships and reducing overfitting by leveraging multiple trees.\n",
    "   - Example: Predicting house prices based on features like size, location, and number of bedrooms.\n",
    "\n",
    "2. **Random Forest Classification**  \n",
    "   - In classification tasks, the algorithm predicts categorical labels. Each decision tree votes on the class, and the final prediction is determined by majority voting across the trees.\n",
    "   - It is robust to noise and handles imbalanced datasets effectively.\n",
    "   - Example: Classifying emails as spam or not based on text features.\n",
    "\n",
    "Random Forest is popular because it combines accuracy, interpretability, and resistance to overfitting while working well with high-dimensional data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
